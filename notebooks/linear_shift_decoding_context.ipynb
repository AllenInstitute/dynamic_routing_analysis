{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import ensemble, svm\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "import npc_lims\n",
    "from npc_sessions import DynamicRoutingSession\n",
    "import npc_sessions.utils\n",
    "from dynamic_routing_analysis import spike_utils, decoding_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ephys_sessions = tuple(s for s in npc_lims.get_session_info(is_ephys=True, is_uploaded=True, is_annotated=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_facemap_data(session,session_info,trials,vid_angle,keep_n_SVDs=500,use_s3=True):\n",
    "\n",
    "    vid_angle_npc_names={\n",
    "            'behavior':'side',\n",
    "            'face':'front',\n",
    "            'eye':'eye',\n",
    "            }\n",
    "    \n",
    "    # path=os.path.join(session_info.allen_path,'processed')\n",
    "\n",
    "    if use_s3==False:\n",
    "        if vid_angle=='behavior':\n",
    "            multi_ROI_path=r\"D:\\DR Pilot Data\\full_video_multi_ROI\"\n",
    "            _dir,vidfilename=os.path.split(glob.glob(os.path.join(session_info.allen_path,\"Behavior_*.mp4\"))[0])\n",
    "        elif vid_angle=='face':\n",
    "            multi_ROI_path=r\"D:\\DR Pilot Data\\full_video_multi_ROI_face\"\n",
    "            _dir,vidfilename=os.path.split(glob.glob(os.path.join(session_info.allen_path,\"Face_*.mp4\"))[0])\n",
    "\n",
    "        behav_path = os.path.join(multi_ROI_path,vidfilename[:-4]+'_trimmed_proc.npy')\n",
    "        behav_info=np.load(behav_path,allow_pickle=True)\n",
    "\n",
    "        for frame_time in session._video_frame_times:\n",
    "            if vid_angle_npc_names[vid_angle] in frame_time.name:\n",
    "                cam_frames=frame_time.timestamps\n",
    "                break\n",
    "\n",
    "        facemap_info={}\n",
    "\n",
    "        #actually keep all ROIs\n",
    "        #facemap_info['motion']=behav_info.item()['motion']\n",
    "        facemap_info['motSVD']=behav_info.item()['motSVD']\n",
    "    #use s3 data\n",
    "    else:\n",
    "        # behav_info = np.load(\n",
    "        #     npc_lims.get_cache_path('facemap',session.id,version='any'),\n",
    "        #     allow_pickle=True\n",
    "        # )\n",
    "        # behav_info = behav_info.item()\n",
    "        # cam_frames = behav_info['cam_frames']\n",
    "        camera_to_facemap_name = {\n",
    "            \"face\": \"Face\",\n",
    "            \"behavior\": \"Behavior\",\n",
    "        }\n",
    "        motion_svd = npc_sessions.utils.get_facemap_output_from_s3(\n",
    "                    session.id, camera_to_facemap_name[vid_angle], \"motSVD\"\n",
    "                )\n",
    "        \n",
    "        for frame_time in session._video_frame_times:\n",
    "            if vid_angle_npc_names[vid_angle] in frame_time.name:\n",
    "                cam_frames=frame_time.timestamps\n",
    "                break\n",
    "\n",
    "        facemap_info = {\n",
    "            #'motion': behav_info['motion'],\n",
    "            'motSVD': motion_svd\n",
    "        }\n",
    "\n",
    "    #calculate mean face motion, SVD in 1 sec prior to each trial\n",
    "    # 1 sec before stimulus onset\n",
    "    time_before=0.2\n",
    "    time_after=0\n",
    "    fps=60\n",
    "\n",
    "    behav_SVD_by_trial={}\n",
    "    behav_motion_by_trial={}\n",
    "    mean_trial_behav_SVD={}\n",
    "    mean_trial_behav_motion={}\n",
    "\n",
    "    # trials=pd.read_parquet(\n",
    "    #             npc_lims.get_cache_path('trials',session.id,version='any')\n",
    "    #         )\n",
    "\n",
    "    if use_s3==False:\n",
    "        for rr in range(0,len(facemap_info['motSVD'])):\n",
    "            behav_SVD_by_trial[rr] = np.zeros((int((time_before+time_after)*fps),keep_n_SVDs,len(trials)))\n",
    "            behav_motion_by_trial[rr] = np.zeros((int((time_before+time_after)*fps),len(trials)))\n",
    "\n",
    "            behav_SVD_by_trial[rr][:]=np.nan\n",
    "            behav_motion_by_trial[rr][:]=np.nan\n",
    "\n",
    "            for tt,stimStartTime in enumerate(trials[:]['stim_start_time']):\n",
    "                if len(np.where(cam_frames>=stimStartTime)[0])>0:\n",
    "                    stim_start_frame=np.where(cam_frames>=stimStartTime)[0][0]\n",
    "                    trial_start_frame=int(stim_start_frame-time_before*fps)\n",
    "                    trial_end_frame=int(stim_start_frame+time_after*fps)\n",
    "                    if trial_start_frame<facemap_info['motSVD'][rr][:,0].shape[0] and trial_end_frame<facemap_info['motSVD'][rr][:,0].shape[0]:\n",
    "                        behav_SVD_by_trial[rr][:,:,tt] = facemap_info['motSVD'][rr][trial_start_frame:trial_end_frame,:keep_n_SVDs]    \n",
    "                        behav_motion_by_trial[rr][:,tt] = facemap_info['motion'][rr][trial_start_frame:trial_end_frame]\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "            mean_trial_behav_SVD[rr] = np.nanmean(behav_SVD_by_trial[rr],axis=0)\n",
    "            mean_trial_behav_motion[rr] = np.nanmean(behav_motion_by_trial[rr],axis=0)\n",
    "\n",
    "    else:\n",
    "        rr=0\n",
    "        motsvd=np.asarray(facemap_info['motSVD'][:,:])\n",
    "\n",
    "        behav_SVD_by_trial[rr] = np.zeros((int((time_before+time_after)*fps),keep_n_SVDs,len(trials)))\n",
    "        behav_motion_by_trial[rr] = np.zeros((int((time_before+time_after)*fps),len(trials)))\n",
    "\n",
    "        behav_SVD_by_trial[rr][:]=np.nan\n",
    "        behav_motion_by_trial[rr][:]=np.nan\n",
    "\n",
    "        for tt,stimStartTime in enumerate(trials[:]['stim_start_time']):\n",
    "            if len(np.where(cam_frames>=stimStartTime)[0])>0:\n",
    "                stim_start_frame=np.where(cam_frames>=stimStartTime)[0][0]\n",
    "                trial_start_frame=int(stim_start_frame-time_before*fps)\n",
    "                trial_end_frame=int(stim_start_frame+time_after*fps)\n",
    "                if trial_start_frame<motsvd[:,0].shape[0] and trial_end_frame<motsvd[:,0].shape[0]:\n",
    "                    behav_SVD_by_trial[rr][:,:,tt] = motsvd[trial_start_frame:trial_end_frame,:keep_n_SVDs]    \n",
    "                    # behav_motion_by_trial[rr][:,tt] = facemap_info['motion'][trial_start_frame:trial_end_frame]\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        mean_trial_behav_SVD[rr] = np.nanmean(behav_SVD_by_trial[rr],axis=0)\n",
    "        # mean_trial_behav_motion[rr] = np.nanmean(behav_motion_by_trial[rr],axis=0)\n",
    "\n",
    "    return mean_trial_behav_SVD #mean_trial_behav_motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath=r\"\\\\allen\\programs\\mindscope\\workgroups\\templeton\\TTOC\\decoding results\\new_annotations\\linear_shift_facemap_face_use_more_trials_20_svds\"\n",
    "\n",
    "# decoder_results={}\n",
    "except_dict={}\n",
    "\n",
    "input_data_type='facemap' #spikes or facemap\n",
    "vid_angle='face' #behavior, face, eye\n",
    "central_section='4_blocks_plus'\n",
    "\n",
    "exclude_cue_trials=False\n",
    "##TODO: decode the labels from the previous trial - \n",
    "#### UPDATE: not doing this, can just pick +1 as the \"center\" - otherwise it gets confusing\n",
    "\n",
    "n_unit_threshold=20\n",
    "keep_n_SVDs=20\n",
    "\n",
    "#set params\n",
    "spikes_binsize=0.2\n",
    "spikes_time_before=0.2\n",
    "spikes_time_after=0.1\n",
    "\n",
    "decoder_binsize=0.2\n",
    "decoder_time_before=0.2\n",
    "decoder_time_after=0.1\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "#loop through sessions\n",
    "# session= all_ephys_sessions[10]\n",
    "for session_info in all_ephys_sessions[:]:\n",
    "    session=DynamicRoutingSession(session_info.id)\n",
    "    session_id=str(session_info.id)\n",
    "    decoder_results={}\n",
    "    try:\n",
    "        #load trials and units\n",
    "        try:\n",
    "            trials=pd.read_parquet(\n",
    "                npc_lims.get_cache_path('trials',session_info.id,version='0.0.214')\n",
    "            )\n",
    "        except:\n",
    "            print('no cached trials table, using npc_sessions')\n",
    "            trials = session.trials[:]\n",
    "\n",
    "        if exclude_cue_trials:\n",
    "            trials=trials.query('is_reward_scheduled==False').reset_index()\n",
    "\n",
    "        if input_data_type=='spikes':\n",
    "            #make data array\n",
    "            try:\n",
    "                units=pd.read_parquet(\n",
    "                    npc_lims.get_cache_path('units',session_info.id,version='0.0.214')\n",
    "                )\n",
    "            except:\n",
    "                print('no cached units table, using npc_sessions')\n",
    "                units = session.units[:]\n",
    "            #add probe to structure name\n",
    "            structure_probe=spike_utils.get_structure_probe(units)\n",
    "            for uu, unit in units.iterrows():\n",
    "                units.loc[units['unit_id']==unit['unit_id'],'structure']=structure_probe.loc[structure_probe['unit_id']==unit['unit_id'],'structure_probe']\n",
    "            \n",
    "            #make trial data array for baseline activity\n",
    "            trial_da = spike_utils.make_neuron_time_trials_tensor(units, trials, spikes_time_before, spikes_time_after, spikes_binsize)\n",
    "\n",
    "        elif input_data_type=='facemap':\n",
    "            # mean_trial_behav_SVD,mean_trial_behav_motion = load_facemap_data(session,session_info,trials,vid_angle)\n",
    "            mean_trial_behav_SVD = load_facemap_data(session,session_info,trials,vid_angle,keep_n_SVDs)\n",
    "        \n",
    "        \n",
    "        #make fake blocks for templeton sessions\n",
    "        if 'Templeton' in session_info.project:\n",
    "            start_time=trials['start_time'].iloc[0]\n",
    "            fake_context=np.full(len(trials), fill_value='nan')\n",
    "            fake_block_nums=np.full(len(trials), fill_value=np.nan)\n",
    "            block_context_names=['vis','aud']\n",
    "\n",
    "            if np.random.choice(block_context_names,1)=='vis':\n",
    "                block_contexts=['vis','aud','vis','aud','vis','aud']\n",
    "            else:\n",
    "                block_contexts=['aud','vis','aud','vis','aud','vis']\n",
    "\n",
    "            for block in range(0,6):\n",
    "                block_start_time=start_time+block*10*60\n",
    "                block_end_time=start_time+(block+1)*10*60\n",
    "                block_trials=trials[:].query('start_time>=@block_start_time').index\n",
    "                fake_context[block_trials]=block_contexts[block]\n",
    "                fake_block_nums[block_trials]=block\n",
    "            trials['block_index']=fake_block_nums\n",
    "            trials['context_name']=fake_context\n",
    "\n",
    "        if central_section=='4_blocks':\n",
    "            #find middle 4 block labels\n",
    "            middle_4_block_trials=trials.query('block_index>0 and block_index<5')\n",
    "            middle_4_blocks=middle_4_block_trials.index.values\n",
    "\n",
    "            #find the number of trials to shift by, from -1 to +1 block\n",
    "            negative_shift=middle_4_blocks.min()\n",
    "            positive_shift=trials.index.max()-middle_4_blocks.max()\n",
    "            shifts=np.arange(-negative_shift,positive_shift+1)\n",
    "        elif central_section=='4_blocks_plus':\n",
    "            #find middle 4 block labels\n",
    "            first_block=trials.query('block_index==0').index.values\n",
    "            middle_of_first=first_block[np.round(len(first_block)/2).astype('int')]\n",
    "\n",
    "            last_block=trials.query('block_index==5').index.values\n",
    "            middle_of_last=last_block[np.round(len(last_block)/2).astype('int')]\n",
    "\n",
    "            middle_4_block_trials=trials.loc[middle_of_first:middle_of_last]\n",
    "            middle_4_blocks=middle_4_block_trials.index.values\n",
    "\n",
    "            #find the number of trials to shift by, from -1 to +1 block\n",
    "            negative_shift=middle_4_blocks.min()\n",
    "            positive_shift=trials.index.max()-middle_4_blocks.max()\n",
    "            shifts=np.arange(-negative_shift,positive_shift+1)\n",
    "            # #add 1 block to the end\n",
    "            # shifts=np.concatenate([shifts,[shifts.max()+1]])\n",
    "\n",
    "        decoder_results[session_id]={}\n",
    "        decoder_results[session_id]['shifts'] = shifts\n",
    "        decoder_results[session_id]['middle_4_blocks'] = middle_4_blocks\n",
    "        decoder_results[session_id]['spikes_binsize'] = spikes_binsize\n",
    "        decoder_results[session_id]['spikes_time_before'] = spikes_time_before\n",
    "        decoder_results[session_id]['spikes_time_after'] = spikes_time_after\n",
    "        decoder_results[session_id]['decoder_binsize'] = decoder_binsize\n",
    "        decoder_results[session_id]['decoder_time_before'] = decoder_time_before\n",
    "        decoder_results[session_id]['decoder_time_after'] = decoder_time_after\n",
    "        decoder_results[session_id]['input_data_type'] = input_data_type\n",
    "        if input_data_type=='facemap':\n",
    "            decoder_results[session_id]['vid_angle'] = vid_angle\n",
    "        decoder_results[session_id]['trials'] = trials\n",
    "        decoder_results[session_id]['results'] = {}\n",
    "\n",
    "        \n",
    "        if input_data_type=='spikes':\n",
    "            areas=units['structure'].unique()\n",
    "            areas=np.concatenate([areas,['all']])\n",
    "        elif input_data_type=='facemap':\n",
    "            # areas = list(mean_trial_behav_SVD.keys())\n",
    "            areas=[0]\n",
    "\n",
    "        decoder_results[session_id]['areas'] = areas\n",
    "\n",
    "        for aa in areas:\n",
    "            #make shifted trial data array\n",
    "            if input_data_type=='spikes':\n",
    "                if aa == 'all':\n",
    "                    area_units=units\n",
    "                else:\n",
    "                    area_units=units.query('structure==@aa')\n",
    "\n",
    "                n_units=len(area_units)\n",
    "                if n_units<n_unit_threshold:\n",
    "                    continue\n",
    "            \n",
    "            decoder_results[session_id]['results'][aa]={}\n",
    "            decoder_results[session_id]['results'][aa]['shift']={}\n",
    "\n",
    "            if input_data_type=='spikes':\n",
    "                \n",
    "                decoder_results[session_id]['results'][aa]['unit_ids']={}\n",
    "                decoder_results[session_id]['results'][aa]['n_units']={}\n",
    "                decoder_results[session_id]['results'][aa]['unit_ids']=area_units['unit_id'].values\n",
    "                decoder_results[session_id]['results'][aa]['n_units']=len(area_units)\n",
    "\n",
    "                #find mean ccf location of units\n",
    "                decoder_results[session_id]['results'][aa]['ccf_ap_mean']=area_units['ccf_ap'].mean()\n",
    "                decoder_results[session_id]['results'][aa]['ccf_dv_mean']=area_units['ccf_dv'].mean()\n",
    "                decoder_results[session_id]['results'][aa]['ccf_ml_mean']=area_units['ccf_ml'].mean()\n",
    "\n",
    "            # elif input_data_type=='facemap':\n",
    "            #     # decoder_results[session_id]['results'][aa]['unit_ids']={}\n",
    "            #     # decoder_results[session_id]['results'][aa]['n_units']={}\n",
    "            #     # decoder_results[session_id]['results'][aa]['unit_ids']=list(mean_trial_behav_SVD.keys())\n",
    "            #     # decoder_results[session_id]['results'][aa]['n_units']=len(mean_trial_behav_SVD)\n",
    "\n",
    "            #loop through shifts\n",
    "\n",
    "            for sh,shift in enumerate(shifts):\n",
    "                \n",
    "                labels=middle_4_block_trials['context_name'].values\n",
    "\n",
    "                if input_data_type=='spikes':\n",
    "                    # if exclude_cue_trials:\n",
    "                    shifted_trial_da = trial_da.sel(trials=middle_4_blocks+shift,unit_id=area_units['unit_id'].values).mean(dim='time').values\n",
    "                    # else:\n",
    "                    #     # # use next trial's activity as input / a.k.a. last trial's label\n",
    "                    #     # if np.any((middle_4_blocks+1)+shift > trial_da['trials'].max().values):\n",
    "                    #     #     continue\n",
    "                    #     shifted_trial_da = trial_da.sel(trials=(middle_4_blocks)+shift,unit_id=area_units['unit_id'].values).mean(dim='time').values\n",
    "                    input_data=shifted_trial_da.T\n",
    "\n",
    "                elif input_data_type=='facemap':\n",
    "                    # if exclude_cue_trials:\n",
    "                    trials_used=middle_4_blocks+shift\n",
    "                    # else:\n",
    "                    #     # # use next trial's activity as input / a.k.a. last trial's label\n",
    "                    #     # if np.any((middle_4_blocks+1)+shift > trial_da['trials'].max().values):\n",
    "                    #     #     continue\n",
    "                    #     trials_used=(middle_4_blocks)+shift\n",
    "                    shift_exists=[]\n",
    "                    for tt in trials_used:\n",
    "                        if tt<mean_trial_behav_SVD[aa].shape[1]:\n",
    "                            shift_exists.append(True)\n",
    "                        else:\n",
    "                            shift_exists.append(False)\n",
    "                    shift_exists=np.array(shift_exists)\n",
    "                    trials_used=trials_used[shift_exists]\n",
    "\n",
    "                    SVD=mean_trial_behav_SVD[aa][:,trials_used]\n",
    "                    input_data=SVD.T\n",
    "\n",
    "                    if np.sum(np.isnan(input_data))>0:\n",
    "                        incl_inds=~np.isnan(input_data).any(axis=1)\n",
    "                        input_data=input_data[incl_inds,:]\n",
    "                        labels=labels[incl_inds]\n",
    "\n",
    "                decoder_results[session_id]['results'][aa]['shift'][sh]=decoding_utils.linearSVC_decoder(\n",
    "                        input_data=input_data,\n",
    "                        labels=labels,\n",
    "                        crossval='5_fold',\n",
    "                        crossval_index=None,\n",
    "                        labels_as_index=True\n",
    "                    )\n",
    "\n",
    "            \n",
    "            print(f'finished {session_id} {aa}')\n",
    "        #save results\n",
    "        with open(os.path.join(savepath,session_id+'_decoder_results.pkl'),'wb') as f:\n",
    "            pickle.dump(decoder_results[session_id],f)\n",
    "\n",
    "        print(f'finished {session_id}')\n",
    "        print(f'time elapsed: {time.time()-start_time}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'error in {session_id}')\n",
    "        print(e)\n",
    "        except_dict[session_id]=e\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# except_dict_first_half=except_dict.copy()\n",
    "# except_dict_first_half\n",
    "trial_da['trials'].max().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trials.query('is_reward_scheduled')\n",
    "except_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ephys_sessions[66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "#check that SVDs are aligned - usually see changes arounf stimuli and especially licking\n",
    "#plot SVD aligned to licks and/or stim onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_trial_behav_SVD[aa].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session._facemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find middle 4 block labels\n",
    "first_block=trials.query('block_index==0').index.values\n",
    "middle_of_first=first_block[np.round(len(first_block)/2).astype('int')]\n",
    "\n",
    "last_block=trials.query('block_index==5').index.values\n",
    "middle_of_last=last_block[np.round(len(last_block)/2).astype('int')]\n",
    "\n",
    "middle_4_block_trials=trials.loc[middle_of_first:middle_of_last]\n",
    "middle_4_blocks=middle_4_block_trials.index.values\n",
    "\n",
    "#find the number of trials to shift by, from -1 to +1 block\n",
    "negative_shift=middle_4_blocks.min()\n",
    "positive_shift=trials.index.max()-middle_4_blocks.max()\n",
    "shifts=np.arange(-negative_shift,positive_shift+1)\n",
    "#add 1 block to the end\n",
    "shifts=np.concatenate([shifts,[shifts.max()+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_of_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1)\n",
    "ax.plot(middle_4_block_trials['context_name'].values=='vis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_results[list(decoder_results.keys())[0]].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through decoder results, append to dict\n",
    "files=glob.glob(os.path.join(savepath,'*_decoder_results.pkl'))\n",
    "\n",
    "decoder_results={}\n",
    "for ii,ff in enumerate(files):\n",
    "    with open(ff,'rb') as f:\n",
    "        data=pickle.load(f)\n",
    "    decoder_results[ff.split('\\\\')[-1].split('_decoder')[0]]=data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath=r\"\\\\allen\\programs\\mindscope\\workgroups\\templeton\\TTOC\\decoding results\\linear_shift\"\n",
    "#save results\n",
    "with open(os.path.join(savepath,'combined','DR_behavior_video_test_decoder_results.pkl'),'wb') as f:\n",
    "    pickle.dump(decoder_results,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadpath=r\"\\\\allen\\programs\\mindscope\\workgroups\\templeton\\TTOC\\decoding results\\linear_shift\\combined\\DR_face_video_test_decoder_results.pkl\"\n",
    "with open(loadpath,'rb') as f:\n",
    "    decoder_results=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# areas=units['structure'].unique()\n",
    "# areas=np.concatenate([areas,['all']])\n",
    "# areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units['structure'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id=list(decoder_results.keys())[0]\n",
    "\n",
    "shifts=decoder_results[session_id]['shifts']\n",
    "areas=decoder_results[session_id]['areas']\n",
    "\n",
    "half_neg_shift=np.round(shifts.min()/2)\n",
    "half_pos_shift=np.round(shifts.max()/2)\n",
    "# half_shifts=np.arange(-half_neg_shift,half_pos_shift+1)\n",
    "half_neg_shift_ind=np.where(shifts==half_neg_shift)[0][0]\n",
    "half_pos_shift_ind=np.where(shifts==half_pos_shift)[0][0]\n",
    "half_shift_inds=np.arange(half_neg_shift_ind,half_pos_shift_ind+1)\n",
    "\n",
    "bal_acc={}\n",
    "for aa in areas:\n",
    "    if aa in decoder_results[session_id]['results']:\n",
    "        bal_acc[aa]=[]\n",
    "        for sh in half_shift_inds:\n",
    "            bal_acc[aa].append(decoder_results[session_id]['results'][aa]['shift'][sh]['balanced_accuracy'])\n",
    "        bal_acc[aa]=np.array(bal_acc[aa])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_results[session_id]['results'][aa]['shift'][sh]['balanced_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_acc[aa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts\n",
    "shifts[half_shift_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for aa in areas:\n",
    "    if aa in decoder_results[session_id]['results']:\n",
    "        true_acc=bal_acc[aa][shifts[half_shift_inds]==0]\n",
    "        pval=np.round(np.mean(bal_acc[aa]>=true_acc),decimals=4)\n",
    "        \n",
    "        fig,ax=plt.subplots(1,1)\n",
    "        ax.axhline(true_acc,color='k',linestyle='--',alpha=0.5)\n",
    "        # ax.plot(shifts,bal_acc[aa])\n",
    "        ax.plot(shifts[half_shift_inds],bal_acc[aa])\n",
    "        ax.set_xlabel('trial shift')\n",
    "        ax.set_ylabel('balanced accuracy')\n",
    "        # ax.set_title(aa+' n='+str(decoder_results[session_id]['results'][aa]['n_units'])+' p='+str(pval))\n",
    "        ax.set_title(str(aa)+' p='+str(pval))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributions of null values vs. true value\n",
    "for aa in areas:\n",
    "    if aa in decoder_results[session_id]['results']:\n",
    "        true_acc=bal_acc[aa][shifts[half_shift_inds]==0]\n",
    "        pval=np.round(np.mean(bal_acc[aa]>=true_acc),decimals=4)\n",
    "        pct_95=np.percentile(bal_acc[aa],95)\n",
    "\n",
    "        fig,ax=plt.subplots(1,1)\n",
    "        ax.axvline(true_acc,color='r',linestyle='--',alpha=0.5)\n",
    "        ax.axvline(pct_95,color='k',linestyle='--',alpha=0.5)\n",
    "        \n",
    "        # ax.plot(shifts,bal_acc[aa])\n",
    "        ax.hist(bal_acc[aa],bins=20)\n",
    "        ax.axvline(np.median(bal_acc[aa]),color='k')\n",
    "        ax.set_xlabel('balanced accuracy')\n",
    "        ax.set_ylabel('count')\n",
    "        # ax.set_title(aa+' n='+str(decoder_results[session_id]['results'][aa]['n_units'])+' p='+str(pval))\n",
    "        ax.set_title(str(aa)+' p='+str(pval))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_acc=bal_acc['ACAd'][shifts==0]\n",
    "np.mean(bal_acc['ACAd']>=true_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_trial_da.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_results[session.id]['results'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_4_blocks=trials.query('block_index>0 and block_index<5').index.values\n",
    "trials.index.max()-middle_4_blocks.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# middle_4_blocks.min()\n",
    "trials.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts=np.arange(-91,83+1)\n",
    "middle_4_blocks+shifts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_list=list(decoder_results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_results[session_list[0]].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_results[session_list[5]]['shifts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_list=list(decoder_results.keys())\n",
    "\n",
    "all_bal_acc={}\n",
    "linear_shift_dict={\n",
    "    'session_id':[],\n",
    "    'area':[],\n",
    "    'true_accuracy':[],\n",
    "    'null_accuracy_mean':[],\n",
    "    'null_accuracy_median':[],\n",
    "    'null_accuracy_std':[],\n",
    "    'p_value':[],\n",
    "}\n",
    "\n",
    "#loop through sessions\n",
    "for session_id in session_list:\n",
    "    all_bal_acc[session_id]={}\n",
    "\n",
    "    shifts=decoder_results[session_id]['shifts']\n",
    "    #extract results according to the trial shift\n",
    "    half_neg_shift=np.round(shifts.min()/2)\n",
    "    half_pos_shift=np.round(shifts.max()/2)\n",
    "    # half_shifts=np.arange(-half_neg_shift,half_pos_shift+1)\n",
    "    half_neg_shift_ind=np.where(shifts==half_neg_shift)[0][0]\n",
    "    half_pos_shift_ind=np.where(shifts==half_pos_shift)[0][0]\n",
    "    half_shift_inds=np.arange(half_neg_shift_ind,half_pos_shift_ind+1)\n",
    "\n",
    "    all_bal_acc[session_id]['shifts']=shifts\n",
    "    all_bal_acc[session_id]['half_shift_inds']=half_shift_inds\n",
    "    half_shifts=shifts[half_shift_inds]\n",
    "    \n",
    "    areas=decoder_results[session_id]['areas']\n",
    "\n",
    "    #save balanced accuracy by shift\n",
    "    for aa in areas:\n",
    "        if aa in decoder_results[session_id]['results']:\n",
    "            all_bal_acc[session_id][aa]=[]\n",
    "            for sh in half_shift_inds:\n",
    "                all_bal_acc[session_id][aa].append(decoder_results[session_id]['results'][aa]['shift'][sh]['balanced_accuracy'])\n",
    "            all_bal_acc[session_id][aa]=np.array(all_bal_acc[session_id][aa])\n",
    "\n",
    "            if type(aa)==str:\n",
    "                if '_probe' in aa:\n",
    "                    area_name=aa.split('_probe')[0]\n",
    "                else:\n",
    "                    area_name=aa\n",
    "            else:\n",
    "                area_name=aa\n",
    "            \n",
    "\n",
    "            true_acc_ind=np.where(half_shifts==0)[0][0]\n",
    "            null_acc_ind=np.where(half_shifts!=0)[0]\n",
    "            true_accuracy=all_bal_acc[session_id][aa][true_acc_ind]\n",
    "            null_accuracy_mean=np.mean(all_bal_acc[session_id][aa][null_acc_ind])\n",
    "            null_accuracy_median=np.median(all_bal_acc[session_id][aa][null_acc_ind])\n",
    "            null_accuracy_std=np.std(all_bal_acc[session_id][aa][null_acc_ind])\n",
    "            p_value=np.mean(all_bal_acc[session_id][aa][null_acc_ind]>=true_accuracy)\n",
    "\n",
    "            #make big dict/dataframe for this:\n",
    "            #save true decoding, mean/median null decoding, and p value for each area/probe\n",
    "            linear_shift_dict['session_id'].append(session_id)\n",
    "            linear_shift_dict['area'].append(area_name)\n",
    "            linear_shift_dict['true_accuracy'].append(true_accuracy)\n",
    "            linear_shift_dict['null_accuracy_mean'].append(null_accuracy_mean)\n",
    "            linear_shift_dict['null_accuracy_median'].append(null_accuracy_median)\n",
    "            linear_shift_dict['null_accuracy_std'].append(null_accuracy_std)\n",
    "            linear_shift_dict['p_value'].append(p_value)\n",
    "    \n",
    "\n",
    "linear_shift_df=pd.DataFrame(linear_shift_dict)\n",
    "# linear_shift_df.to_csv(os.path.join(r'\\\\allen\\programs\\mindscope\\workgroups\\templeton\\TTOC\\decoding results\\linear_shift\\combined','Templ_linear_shift_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_shift_df=pd.read_csv(os.path.join(r'\\\\allen\\programs\\mindscope\\workgroups\\templeton\\TTOC\\decoding results\\linear_shift\\combined','DR_linear_shift_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_shift_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_shift_df['area'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#area - fraction significant\n",
    "p_threshold=0.05\n",
    "frac_sig={\n",
    "    'area':[],\n",
    "    'frac_sig':[],\n",
    "    'n_expts':[],\n",
    "}\n",
    "\n",
    "for area in linear_shift_df['area'].unique():\n",
    "    frac_sig['area'].append(area)\n",
    "    frac_sig['frac_sig'].append(np.mean(linear_shift_df.query('area==@area')['p_value']<p_threshold))\n",
    "    frac_sig['n_expts'].append(len(linear_shift_df.query('area==@area')))\n",
    "\n",
    "frac_sig_df=pd.DataFrame(frac_sig)\n",
    "frac_sig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot sorted by fraction significant\n",
    "min_n_expts=3\n",
    "\n",
    "plot_frac_sig_df=frac_sig_df.query('n_expts>=@min_n_expts').sort_values('frac_sig',ascending=False)\n",
    "fig,ax=plt.subplots(1,1,figsize=(12,5))\n",
    "ax.bar(plot_frac_sig_df['area'],plot_frac_sig_df['frac_sig'])\n",
    "ax.set_ylabel('fraction significant decoding')\n",
    "ax.set_xlabel('area')\n",
    "# #add labels with n_expts\n",
    "# area_labels=[]\n",
    "# for i in range(plot_frac_sig_df.shape[0]):\n",
    "#     area_labels.append(plot_frac_sig_df['area'].iloc[i]+' ('+str(plot_frac_sig_df['n_expts'].iloc[i])+')')\n",
    "# ax.set_xticklabels(area_labels,rotation=90,ha='center')\n",
    "ax.set_ylim([0,1])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#area - diff from null\n",
    "diff_from_null={\n",
    "    'area':[],\n",
    "    'diff_from_null_mean':[],\n",
    "    'diff_from_null_median':[],\n",
    "    'n_expts':[],\n",
    "}\n",
    "\n",
    "for area in linear_shift_df['area'].unique():\n",
    "    diff_from_null['area'].append(area)\n",
    "    diff_from_null['diff_from_null_mean'].append((linear_shift_df.query('area==@area')['true_accuracy']-\n",
    "                                                 linear_shift_df.query('area==@area')['null_accuracy_mean']).mean())\n",
    "    diff_from_null['diff_from_null_median'].append((linear_shift_df.query('area==@area')['true_accuracy']-\n",
    "                                                    linear_shift_df.query('area==@area')['null_accuracy_median']).mean())\n",
    "    diff_from_null['n_expts'].append(len(linear_shift_df.query('area==@area')))\n",
    "\n",
    "diff_from_null_df=pd.DataFrame(diff_from_null)\n",
    "diff_from_null_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot sorted by diff from null mean\n",
    "min_n_expts=3\n",
    "\n",
    "plot_diff_from_null_df=diff_from_null_df.query('n_expts>=@min_n_expts').sort_values('diff_from_null_mean',ascending=False) \n",
    "fig,ax=plt.subplots(1,1,figsize=(12,5))\n",
    "ax.bar(plot_diff_from_null_df['area'],plot_diff_from_null_df['diff_from_null_mean'])\n",
    "ax.set_ylabel('mean difference from null')\n",
    "ax.set_xlabel('area')\n",
    "#add labels with n_expts\n",
    "# area_labels=[]\n",
    "# for i in range(plot_diff_from_null_df.shape[0]):\n",
    "#     area_labels.append(plot_diff_from_null_df['area'].iloc[i]+' ('+str(plot_diff_from_null_df['n_expts'].iloc[i])+')')\n",
    "# ax.set_xticklabels(area_labels,rotation=90,ha='center')\n",
    "ax.set_ylim([-0.025,0.12])\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot sorted by diff from null median\n",
    "min_n_expts=3\n",
    "\n",
    "plot_diff_from_null_df=diff_from_null_df.query('n_expts>=@min_n_expts').sort_values('diff_from_null_median',ascending=False)\n",
    "fig,ax=plt.subplots(1,1,figsize=(12,5))\n",
    "ax.bar(plot_diff_from_null_df['area'],plot_diff_from_null_df['diff_from_null_median'])\n",
    "ax.set_ylabel('median difference from null')\n",
    "ax.set_xlabel('area')\n",
    "#add labels with n_expts\n",
    "area_labels=[]\n",
    "for i in range(plot_diff_from_null_df.shape[0]):\n",
    "    area_labels.append(plot_diff_from_null_df['area'].iloc[i]+' ('+str(plot_diff_from_null_df['n_expts'].iloc[i])+')')\n",
    "ax.set_xticklabels(area_labels,rotation=90,ha='center')\n",
    "ax.set_ylim([-0.025,0.12])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare DR and Templeton:\n",
    "p_threshold=0.05\n",
    "\n",
    "DR_linear_shift_df=pd.read_csv(os.path.join(r'\\\\allen\\programs\\mindscope\\workgroups\\templeton\\TTOC\\decoding results\\linear_shift\\combined','DR_linear_shift_results.csv'))\n",
    "DR_linear_shift_df['project']='DynamicRouting'\n",
    "#fraction significant\n",
    "frac_sig_DR={\n",
    "    'area':[],\n",
    "    'frac_sig_DR':[],\n",
    "    'n_expts_DR':[],\n",
    "}\n",
    "for area in DR_linear_shift_df['area'].unique():\n",
    "    frac_sig_DR['area'].append(area)\n",
    "    frac_sig_DR['frac_sig_DR'].append(np.mean(DR_linear_shift_df.query('area==@area')['p_value']<p_threshold))\n",
    "    frac_sig_DR['n_expts_DR'].append(len(DR_linear_shift_df.query('area==@area')))\n",
    "frac_sig_DR_df=pd.DataFrame(frac_sig_DR)\n",
    "#diff from null\n",
    "diff_from_null_DR={\n",
    "    'area':[],\n",
    "    'diff_from_null_mean_DR':[],\n",
    "    'diff_from_null_median_DR':[],\n",
    "    'n_expts_DR':[],\n",
    "}\n",
    "for area in DR_linear_shift_df['area'].unique():\n",
    "    diff_from_null_DR['area'].append(area)\n",
    "    diff_from_null_DR['diff_from_null_mean_DR'].append((DR_linear_shift_df.query('area==@area')['true_accuracy']-\n",
    "                                                 DR_linear_shift_df.query('area==@area')['null_accuracy_mean']).mean())\n",
    "    diff_from_null_DR['diff_from_null_median_DR'].append((DR_linear_shift_df.query('area==@area')['true_accuracy']-\n",
    "                                                    DR_linear_shift_df.query('area==@area')['null_accuracy_median']).mean())\n",
    "    diff_from_null_DR['n_expts_DR'].append(len(DR_linear_shift_df.query('area==@area')))\n",
    "\n",
    "diff_from_null_DR_df=pd.DataFrame(diff_from_null_DR)\n",
    "diff_from_null_DR_df\n",
    "\n",
    "\n",
    "Templeton_linear_shift_df=pd.read_csv(os.path.join(r'\\\\allen\\programs\\mindscope\\workgroups\\templeton\\TTOC\\decoding results\\linear_shift\\combined','Templ_linear_shift_results.csv'))\n",
    "Templeton_linear_shift_df['project']='Templeton'\n",
    "#fraction significant\n",
    "frac_sig_Templ={\n",
    "    'area':[],\n",
    "    'frac_sig_Templ':[],\n",
    "    'n_expts_Templ':[],\n",
    "}\n",
    "for area in Templeton_linear_shift_df['area'].unique():\n",
    "    frac_sig_Templ['area'].append(area)\n",
    "    frac_sig_Templ['frac_sig_Templ'].append(np.mean(Templeton_linear_shift_df.query('area==@area')['p_value']<p_threshold))\n",
    "    frac_sig_Templ['n_expts_Templ'].append(len(Templeton_linear_shift_df.query('area==@area')))\n",
    "frac_sig_Templ_df=pd.DataFrame(frac_sig_Templ)\n",
    "#diff from null\n",
    "diff_from_null_Templ={\n",
    "    'area':[],\n",
    "    'diff_from_null_mean_Templ':[],\n",
    "    'diff_from_null_median_Templ':[],\n",
    "    'n_expts_Templ':[],\n",
    "}\n",
    "for area in Templeton_linear_shift_df['area'].unique():\n",
    "    diff_from_null_Templ['area'].append(area)\n",
    "    diff_from_null_Templ['diff_from_null_mean_Templ'].append((Templeton_linear_shift_df.query('area==@area')['true_accuracy']-\n",
    "                                                 Templeton_linear_shift_df.query('area==@area')['null_accuracy_mean']).mean())\n",
    "    diff_from_null_Templ['diff_from_null_median_Templ'].append((Templeton_linear_shift_df.query('area==@area')['true_accuracy']-\n",
    "                                                    Templeton_linear_shift_df.query('area==@area')['null_accuracy_median']).mean())\n",
    "    diff_from_null_Templ['n_expts_Templ'].append(len(Templeton_linear_shift_df.query('area==@area')))\n",
    "diff_from_null_Templ_df=pd.DataFrame(diff_from_null_Templ)\n",
    "\n",
    "\n",
    "all_frac_sig_df=pd.merge(frac_sig_DR_df,frac_sig_Templ_df,on='area',how='outer')\n",
    "all_diff_from_null_df=pd.merge(diff_from_null_DR_df,diff_from_null_Templ_df,on='area',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot sorted by fraction significant\n",
    "min_n_expts=2\n",
    "\n",
    "plot_all_frac_sig_df=all_frac_sig_df.query('n_expts_DR>=@min_n_expts and n_expts_Templ>=@min_n_expts').sort_values('frac_sig_DR',ascending=False)\n",
    "fig,ax=plt.subplots(1,1,figsize=(12,5))\n",
    "plot_all_frac_sig_df.plot.bar(x='area',y=['frac_sig_DR','frac_sig_Templ'],ax=ax)\n",
    "ax.set_ylabel('fraction significant decoding')\n",
    "ax.set_xlabel('area')\n",
    "ax.set_ylim([0,1])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot sorted by DR diff from median\n",
    "min_n_expts=2\n",
    "\n",
    "plot_all_diff_from_null_df=all_diff_from_null_df.query('n_expts_DR>=@min_n_expts and n_expts_Templ>=@min_n_expts').sort_values('diff_from_null_median_DR',ascending=False)\n",
    "fig,ax=plt.subplots(1,1,figsize=(12,5))\n",
    "plot_all_diff_from_null_df.plot.bar(x='area',y=['diff_from_null_median_DR','diff_from_null_median_Templ'],ax=ax)\n",
    "\n",
    "ax.set_ylabel('median difference from null')\n",
    "ax.set_xlabel('area')\n",
    "\n",
    "ax.set_ylim([-0.025,0.12])\n",
    "fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Templeton_linear_shift_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vect=np.arange(0,1.01,0.01)\n",
    "\n",
    "fig,ax=plt.subplots(1,1)\n",
    "ax.hist(1-DR_linear_shift_df['p_value'],bins=x_vect,alpha=0.5)\n",
    "ax.hist(1-Templeton_linear_shift_df['p_value'],bins=x_vect,alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('zero shift percentile')\n",
    "ax.set_ylabel('count (session-areas)')\n",
    "\n",
    "ax.legend(['DR','Templeton'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vect=np.arange(-0.1,0.3,0.01)\n",
    "\n",
    "fig,ax=plt.subplots(1,1)\n",
    "ax.hist(DR_linear_shift_df['true_accuracy']-DR_linear_shift_df['null_accuracy_median'],bins=x_vect)\n",
    "ax.hist(Templeton_linear_shift_df['true_accuracy']-Templeton_linear_shift_df['null_accuracy_median'],bins=x_vect)\n",
    "\n",
    "ax.set_xlabel('difference from null distribution median')\n",
    "ax.set_ylabel('count (session-areas)')\n",
    "\n",
    "ax.legend(['DR','Templeton'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npc_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
