{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import npc_lims, npc_ephys, npc_session\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from dynamic_routing_analysis import spike_utils\n",
    "from npc_sessions import DynamicRoutingSession\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #calculate metrics for channel alignment\n",
    "\n",
    "# def compute_metrics_for_alignment(trials, units, session_info, save_path):\n",
    "\n",
    "#     stim_context_modulation = {\n",
    "#         'unit_id':[],\n",
    "#         'session_id':[],\n",
    "#         'experiment_day':[],\n",
    "#         'project':[],\n",
    "#         'probe':[],\n",
    "#         'peak_channel':[],\n",
    "#         'lick_modulation_roc_auc':[],\n",
    "#         'vis_discrim_roc_auc':[],\n",
    "#         'aud_discrim_roc_auc':[],\n",
    "#         'any_vis_roc_auc':[],\n",
    "#         'any_aud_roc_auc':[],\n",
    "#         'firing_rate':[],\n",
    "#         'peak_to_valley':[],\n",
    "#         'peak_trough_ratio':[],\n",
    "#         'repolarization_slope':[],\n",
    "#         'recovery_slope':[],\n",
    "#         'spread':[],\n",
    "#         'velocity_above':[],\n",
    "#         'velocity_below':[],\n",
    "#         'snr':[],\n",
    "\n",
    "#         'amplitude_cutoff':[], \n",
    "#         'amplitude_cv_median':[], \n",
    "#         'amplitude_cv_range':[],\n",
    "#         'amplitude_median':[], \n",
    "#         'drift_ptp':[], \n",
    "#         'drift_std':[], \n",
    "#         'drift_mad':[],\n",
    "#         'firing_range':[], \n",
    "#         'isi_violations_ratio':[],\n",
    "#         'isi_violations_count':[], \n",
    "#         'presence_ratio':[],\n",
    "#         'rp_contamination':[], \n",
    "#         'rp_violations':[], \n",
    "#         'sliding_rp_violation':[],\n",
    "#         'sync_spike_2':[], \n",
    "#         'sync_spike_4':[], \n",
    "#         'sync_spike_8':[], \n",
    "#         'd_prime':[],\n",
    "#         'isolation_distance':[], \n",
    "#         'l_ratio':[], \n",
    "#         'silhouette':[], \n",
    "#         'nn_hit_rate':[],\n",
    "#         'nn_miss_rate':[], \n",
    "#         'exp_decay':[], \n",
    "#         'half_width':[], \n",
    "#         'num_negative_peaks':[],\n",
    "#         'num_positive_peaks':[],\n",
    "#     }\n",
    "\n",
    "#     if trials is not None:\n",
    "#         contexts=trials['context_name'].unique()\n",
    "\n",
    "#         if 'Templeton' in session_info.project:\n",
    "#             contexts = ['aud','vis']\n",
    "\n",
    "#             start_time=trials['start_time'].iloc[0]\n",
    "#             fake_context=np.full(len(trials), fill_value='nan')\n",
    "#             fake_block_nums=np.full(len(trials), fill_value=np.nan)\n",
    "\n",
    "#             if np.random.choice(contexts,1)=='vis':\n",
    "#                 block_contexts=['vis','aud','vis','aud','vis','aud']\n",
    "#             else:\n",
    "#                 block_contexts=['aud','vis','aud','vis','aud','vis']\n",
    "\n",
    "#             trials['true_block_index']=trials['block_index']\n",
    "#             trials['true_context_name']=trials['context_name']\n",
    "\n",
    "#             for block in range(0,6):\n",
    "#                 block_start_time=start_time+block*10*60\n",
    "#                 block_end_time=start_time+(block+1)*10*60\n",
    "#                 block_trials=trials.query('start_time>=@block_start_time').index\n",
    "#                 fake_context[block_trials]=block_contexts[block]\n",
    "#                 fake_block_nums[block_trials]=block\n",
    "            \n",
    "#             trials['context_name']=fake_context\n",
    "#             trials['block_index']=fake_block_nums\n",
    "#             trials['is_vis_context']=trials['context_name']=='vis'\n",
    "#             trials['is_aud_context']=trials['context_name']=='aud'\n",
    "\n",
    "#         #make data array first\n",
    "#         time_before = 0.5\n",
    "#         time_after = 0.5\n",
    "#         binsize = 0.025\n",
    "#         trial_da = spike_utils.make_neuron_time_trials_tensor(units, trials, time_before, time_after, binsize)\n",
    "\n",
    "#     #for each unit\n",
    "#     for uu,unit in units.iterrows():\n",
    "        \n",
    "#         stim_context_modulation['unit_id'].append(unit['unit_id'])\n",
    "#         stim_context_modulation['session_id'].append(str(session_info.id))\n",
    "#         stim_context_modulation['project'].append(str(session_info.project))\n",
    "#         stim_context_modulation['experiment_day'].append(str(session_info.experiment_day))\n",
    "#         stim_context_modulation['probe'].append(str(unit['electrode_group_name']))\n",
    "#         stim_context_modulation['peak_channel'].append(unit['peak_channel'])\n",
    "\n",
    "#         stim_context_modulation['firing_rate'].append(unit['firing_rate'])\n",
    "#         stim_context_modulation['peak_to_valley'].append(unit['peak_to_valley'])\n",
    "#         stim_context_modulation['peak_trough_ratio'].append(unit['peak_trough_ratio'])\n",
    "#         stim_context_modulation['repolarization_slope'].append(unit['repolarization_slope'])\n",
    "#         stim_context_modulation['recovery_slope'].append(unit['recovery_slope'])\n",
    "\n",
    "#         stim_context_modulation['spread'].append(unit['spread'])\n",
    "#         stim_context_modulation['velocity_above'].append(unit['velocity_above'])\n",
    "#         stim_context_modulation['velocity_below'].append(unit['velocity_below'])\n",
    "#         stim_context_modulation['snr'].append(unit['spread'])\n",
    "\n",
    "#         stim_context_modulation['amplitude_cutoff'].append(unit['amplitude_cutoff'])\n",
    "#         stim_context_modulation['amplitude_cv_median'].append(unit['amplitude_cv_median'])\n",
    "#         stim_context_modulation['amplitude_cv_range'].append(unit['amplitude_cv_range'])\n",
    "#         stim_context_modulation['amplitude_median'].append(unit['amplitude_median'])\n",
    "#         stim_context_modulation['drift_ptp'].append(unit['drift_ptp'])\n",
    "#         stim_context_modulation['drift_std'].append(unit['drift_std'])\n",
    "#         stim_context_modulation['drift_mad'].append(unit['drift_mad'])\n",
    "#         stim_context_modulation['firing_range'].append(unit['firing_range'])\n",
    "#         stim_context_modulation['isi_violations_ratio'].append(unit['isi_violations_ratio'])\n",
    "#         stim_context_modulation['isi_violations_count'].append(unit['isi_violations_count'])\n",
    "#         stim_context_modulation['presence_ratio'].append(unit['presence_ratio'])\n",
    "#         stim_context_modulation['rp_contamination'].append(unit['rp_contamination'])\n",
    "#         stim_context_modulation['rp_violations'].append(unit['rp_violations'])\n",
    "#         stim_context_modulation['sliding_rp_violation'].append(unit['sliding_rp_violation'])\n",
    "#         stim_context_modulation['sync_spike_2'].append(unit['sync_spike_2'])\n",
    "#         stim_context_modulation['sync_spike_4'].append(unit['sync_spike_4'])\n",
    "#         stim_context_modulation['sync_spike_8'].append(unit['sync_spike_8'])\n",
    "#         stim_context_modulation['d_prime'].append(unit['d_prime'])\n",
    "#         stim_context_modulation['isolation_distance'].append(unit['isolation_distance'])\n",
    "#         stim_context_modulation['l_ratio'].append(unit['l_ratio'])\n",
    "#         stim_context_modulation['silhouette'].append(unit['silhouette'])\n",
    "#         stim_context_modulation['nn_hit_rate'].append(unit['nn_hit_rate'])\n",
    "#         stim_context_modulation['nn_miss_rate'].append(unit['nn_miss_rate'])\n",
    "#         stim_context_modulation['exp_decay'].append(unit['exp_decay'])\n",
    "#         stim_context_modulation['half_width'].append(unit['half_width'])\n",
    "#         stim_context_modulation['num_negative_peaks'].append(unit['num_negative_peaks'])\n",
    "#         stim_context_modulation['num_positive_peaks'].append(unit['num_positive_peaks'])\n",
    "        \n",
    "#         #if surface channel, don't try to calculate metrics\n",
    "#         if unit['peak_channel']>383:\n",
    "#             #append nans for all metrics\n",
    "#             stim_context_modulation['any_vis_roc_auc'].append(np.nan)\n",
    "#             stim_context_modulation['any_aud_roc_auc'].append(np.nan)\n",
    "#             stim_context_modulation['vis_discrim_roc_auc'].append(np.nan)\n",
    "#             stim_context_modulation['aud_discrim_roc_auc'].append(np.nan)\n",
    "#             stim_context_modulation['lick_modulation_roc_auc'].append(np.nan)\n",
    "#             continue\n",
    "        \n",
    "#         if trials is not None:\n",
    "#             #find baseline frs across all trials\n",
    "#             baseline_frs = trial_da.sel(unit_id=unit['unit_id'],time=slice(-0.1,0)).mean(dim='time')\n",
    "\n",
    "#             all_stim_frs_by_trial = {}\n",
    "#             #loop through stimuli\n",
    "#             for ss in trials['stim_name'].unique():\n",
    "\n",
    "#                 #stimulus modulation\n",
    "#                 if \"Templeton\" in session_info.project:\n",
    "#                     stim_trials = trials.query('stim_name==@ss')\n",
    "#                 else:\n",
    "#                     stim_trials = trials.query('stim_name==@ss and is_response==False') #remove response trials to minimize contamination\n",
    "#                 stim_frs_by_trial = trial_da.sel(unit_id=unit['unit_id'],time=slice(0,0.1),trials=stim_trials.index).mean(dim='time',skipna=True)\n",
    "\n",
    "#                 all_stim_frs_by_trial[ss]=stim_frs_by_trial\n",
    "\n",
    "#             if \"Templeton\" in session_info.project:\n",
    "#                 any_vis_trials = trials.query('stim_name.str.contains(\"vis\")')\n",
    "#             else:\n",
    "#                 any_vis_trials = trials.query('stim_name.str.contains(\"vis\") and is_response==False')\n",
    "#             any_vis_frs_by_trial = trial_da.sel(unit_id=unit['unit_id'],time=slice(0,0.1),trials=any_vis_trials.index).mean(dim='time',skipna=True)\n",
    "#             any_vis_baseline_frs_by_trial = baseline_frs.sel(trials=any_vis_trials.index)\n",
    "#             any_vis_and_baseline_frs=np.concatenate([any_vis_frs_by_trial.values,any_vis_baseline_frs_by_trial.values])\n",
    "#             binary_label=np.concatenate([np.ones(len(any_vis_frs_by_trial)),np.zeros(len(any_vis_baseline_frs_by_trial))])\n",
    "#             if len(np.unique(binary_label))>1:\n",
    "#                 any_vis_context_auc=roc_auc_score(binary_label,any_vis_and_baseline_frs)\n",
    "#             else:\n",
    "#                 any_vis_context_auc=np.nan\n",
    "#             stim_context_modulation['any_vis_roc_auc'].append(any_vis_context_auc)\n",
    "\n",
    "#             if \"Templeton\" in session_info.project:\n",
    "#                 any_aud_trials = trials.query('stim_name.str.contains(\"sound\")')\n",
    "#             else:\n",
    "#                 any_aud_trials = trials.query('stim_name.str.contains(\"sound\") and is_response==False')\n",
    "#             any_aud_frs_by_trial = trial_da.sel(unit_id=unit['unit_id'],time=slice(0,0.1),trials=any_aud_trials.index).mean(dim='time',skipna=True)\n",
    "#             any_aud_baseline_frs_by_trial = baseline_frs.sel(trials=any_aud_trials.index)\n",
    "#             any_aud_and_baseline_frs=np.concatenate([any_aud_frs_by_trial.values,any_aud_baseline_frs_by_trial.values])\n",
    "#             binary_label=np.concatenate([np.ones(len(any_aud_frs_by_trial)),np.zeros(len(any_aud_baseline_frs_by_trial))])\n",
    "#             if len(np.unique(binary_label))>1:\n",
    "#                 any_aud_context_auc=roc_auc_score(binary_label,any_aud_and_baseline_frs)\n",
    "#             else:\n",
    "#                 any_aud_context_auc=np.nan\n",
    "#             stim_context_modulation['any_aud_roc_auc'].append(any_aud_context_auc)\n",
    "\n",
    "#             #same modality stimulus discrimination\n",
    "#             #vis1 vs. vis2\n",
    "#             vis1_and_vis2_frs=np.concatenate([all_stim_frs_by_trial['vis1'].values,all_stim_frs_by_trial['vis2'].values])\n",
    "#             binary_label=np.concatenate([np.ones(len(all_stim_frs_by_trial['vis1'])),np.zeros(len(all_stim_frs_by_trial['vis2']))])\n",
    "#             if len(np.unique(binary_label))>1:\n",
    "#                 vis_discrim_auc=roc_auc_score(binary_label,vis1_and_vis2_frs)\n",
    "#             else:\n",
    "#                 vis_discrim_auc=np.nan\n",
    "#             stim_context_modulation['vis_discrim_roc_auc'].append(vis_discrim_auc)\n",
    "\n",
    "#             #aud1 vs. aud2\n",
    "#             aud1_and_aud2_frs=np.concatenate([all_stim_frs_by_trial['sound1'].values,all_stim_frs_by_trial['sound2'].values])\n",
    "#             binary_label=np.concatenate([np.ones(len(all_stim_frs_by_trial['sound1'])),np.zeros(len(all_stim_frs_by_trial['sound2']))])\n",
    "#             if len(np.unique(binary_label))>1:\n",
    "#                 aud_discrim_auc=roc_auc_score(binary_label,aud1_and_aud2_frs)\n",
    "#             else:\n",
    "#                 aud_discrim_auc=np.nan\n",
    "#             stim_context_modulation['aud_discrim_roc_auc'].append(aud_discrim_auc)\n",
    "\n",
    "#             # #targets: vis1 vs sound1\n",
    "#             # vis1_vs_aud1_frs=np.concatenate([all_stim_frs_by_trial['vis1'].values,all_stim_frs_by_trial['sound1'].values])\n",
    "#             # binary_label=np.concatenate([np.ones(len(all_stim_frs_by_trial['vis1'])),np.zeros(len(all_stim_frs_by_trial['sound1']))])\n",
    "#             # target_discrim_auc=roc_auc_score(binary_label,vis1_vs_aud1_frs)\n",
    "#             # stim_context_modulation['target_discrim_roc_auc'].append(target_discrim_auc)\n",
    "\n",
    "#             # #nontargets: vis2 vs sound2\n",
    "#             # vis2_vs_aud2_frs=np.concatenate([all_stim_frs_by_trial['vis2'].values,all_stim_frs_by_trial['sound2'].values])\n",
    "#             # binary_label=np.concatenate([np.ones(len(all_stim_frs_by_trial['vis2'])),np.zeros(len(all_stim_frs_by_trial['sound2']))])\n",
    "#             # nontarget_discrim_auc=roc_auc_score(binary_label,vis2_vs_aud2_frs)\n",
    "#             # stim_context_modulation['nontarget_discrim_roc_auc'].append(nontarget_discrim_auc)\n",
    "\n",
    "#             # #vis vs. aud\n",
    "#             # vis_and_aud_frs=np.concatenate([all_stim_frs_by_trial['vis1'].values,all_stim_frs_by_trial['vis2'].values,\n",
    "#             #                                 all_stim_frs_by_trial['sound1'].values,all_stim_frs_by_trial['sound2'].values])\n",
    "#             # binary_label=np.concatenate([np.ones(len(all_stim_frs_by_trial['vis1'])+len(all_stim_frs_by_trial['vis2'])),\n",
    "#             #                             np.zeros(len(all_stim_frs_by_trial['sound1'])+len(all_stim_frs_by_trial['sound2']))])\n",
    "#             # vis_vs_aud_auc=roc_auc_score(binary_label,vis_and_aud_frs)\n",
    "#             # stim_context_modulation['vis_vs_aud_roc_auc'].append(vis_vs_aud_auc)\n",
    "\n",
    "#             #lick modulation\n",
    "#             if \"DynamicRouting\" in session_info.project:\n",
    "#                 lick_trials = trials.query('(stim_name==\"vis1\" and context_name==\"aud\" and is_response==True) or \\\n",
    "#                                         (stim_name==\"sound1\" and context_name==\"vis\" and is_response==True)')\n",
    "#                 non_lick_trials = trials.query('(stim_name==\"vis1\" and context_name==\"aud\" and is_response==False) or \\\n",
    "#                                                 (stim_name==\"sound1\" and context_name==\"vis\" and is_response==False)')\n",
    "#             elif \"Templeton\" in session_info.project:\n",
    "#                 lick_trials = trials.query('is_response==True')\n",
    "#                 non_lick_trials = trials.query('is_response==False')\n",
    "\n",
    "#             lick_frs_by_trial = trial_da.sel(unit_id=unit['unit_id'],time=slice(0.2,0.5),trials=lick_trials.index).mean(dim='time',skipna=True)\n",
    "#             non_lick_frs_by_trial = trial_da.sel(unit_id=unit['unit_id'],time=slice(0.2,0.5),trials=non_lick_trials.index).mean(dim='time',skipna=True)\n",
    "\n",
    "#             #ROC AUC\n",
    "#             binary_label = np.concatenate([np.ones(lick_frs_by_trial.size),np.zeros(non_lick_frs_by_trial.size)])\n",
    "#             binary_score = np.concatenate([lick_frs_by_trial.values,non_lick_frs_by_trial.values])\n",
    "#             if len(np.unique(binary_label))>1:\n",
    "#                 lick_roc_auc = roc_auc_score(binary_label, binary_score)\n",
    "#             else:\n",
    "#                 lick_roc_auc = np.nan\n",
    "#             stim_context_modulation['lick_modulation_roc_auc'].append(lick_roc_auc)\n",
    "        \n",
    "#         else:\n",
    "#             stim_context_modulation['any_vis_roc_auc'].append(np.nan)\n",
    "#             stim_context_modulation['any_aud_roc_auc'].append(np.nan)\n",
    "#             stim_context_modulation['vis_discrim_roc_auc'].append(np.nan)\n",
    "#             stim_context_modulation['aud_discrim_roc_auc'].append(np.nan)\n",
    "#             stim_context_modulation['lick_modulation_roc_auc'].append(np.nan)\n",
    "    \n",
    "#     stim_context_modulation = pd.DataFrame(stim_context_modulation)\n",
    "#     stim_context_modulation['visual_response'] = np.abs(stim_context_modulation['any_vis_roc_auc'] - 0.5)*2\n",
    "#     stim_context_modulation['auditory_response'] = np.abs(stim_context_modulation['any_aud_roc_auc'] - 0.5)*2\n",
    "#     stim_context_modulation['visual_discrim'] = np.abs(stim_context_modulation['vis_discrim_roc_auc'] - 0.5)*2\n",
    "#     stim_context_modulation['auditory_discrim'] = np.abs(stim_context_modulation['aud_discrim_roc_auc'] - 0.5)*2\n",
    "#     stim_context_modulation['lick_modulation'] = np.abs(stim_context_modulation['lick_modulation_roc_auc'] - 0.5)*2\n",
    "\n",
    "\n",
    "#     stim_context_modulation.drop(columns=['any_vis_roc_auc','any_aud_roc_auc','vis_discrim_roc_auc','aud_discrim_roc_auc','lick_modulation_roc_auc'],inplace=True)\n",
    "\n",
    "#     probes=stim_context_modulation['probe'].unique()\n",
    "#     for probe in probes:\n",
    "#         probe_units=stim_context_modulation.query('probe==@probe')\n",
    "#         probe_units.to_csv(os.path.join(save_path,session_info.id+'_day_'+str(session_info.experiment_day)+'_'+probe+'_stim_modulation.csv'),index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys_sessions=tuple(s for s in npc_lims.get_session_info(is_ephys=True, is_uploaded=True,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get units from npc_ephys\n",
    "save_path=r\"\\\\allen\\programs\\mindscope\\workgroups\\dynamicrouting\\Ethan\\metrics for alignment\\stimulus responsiveness\"\n",
    "save_path_wf=r\"\\\\allen\\programs\\mindscope\\workgroups\\dynamicrouting\\Ethan\\metrics for alignment\\peak waveforms\"\n",
    "#fix to try probe-by-probe\n",
    "\n",
    "save_waveforms=True\n",
    "compute_waveform_PCs=True\n",
    "overwrite=False\n",
    "\n",
    "except_dict={}\n",
    "\n",
    "for session_info in ephys_sessions[1:2]:\n",
    "    try:\n",
    "        print(session_info.id)\n",
    "        si = npc_ephys.SpikeInterfaceKS25Data(session_info.id)\n",
    "        #check for surface channel asset\n",
    "        if session_info.is_surface_channels:\n",
    "            surface_channel_session_id=npc_session.SessionRecord(session_info.id).with_idx(1)\n",
    "            sorted_surface_channel_asset = npc_lims.get_session_sorted_data_asset(surface_channel_session_id)\n",
    "            #only use surface channel recordings sorted recently\n",
    "            if '2024' in sorted_surface_channel_asset['name']:\n",
    "                surface_flag=True\n",
    "                si_surface_channels=npc_ephys.SpikeInterfaceKS25Data(surface_channel_session_id)\n",
    "            else:\n",
    "                surface_flag=False\n",
    "                si_surface_channels=None\n",
    "        else:\n",
    "            surface_flag=False\n",
    "            si_surface_channels=None\n",
    "        try:\n",
    "            trials=pd.read_parquet(\n",
    "                                npc_lims.get_cache_path('trials',session_info.id,version='any')\n",
    "                            )\n",
    "        except:\n",
    "             print('no cached trials table, using npc_sessions')\n",
    "             session = DynamicRoutingSession(session_info.id)\n",
    "             trials = session.trials[:]\n",
    "            #  trials=None\n",
    "        for probe in si.probes:\n",
    "            #check for existing file(s)\n",
    "            \n",
    "            try:\n",
    "                if ((os.path.exists(os.path.join(save_path,session_info.id+'_day_'+str(session_info.experiment_day)+'_probe'+probe+'_stim_modulation.csv')) and \n",
    "                    (os.path.exists(os.path.join(save_path_wf,session_info.id+'_day_'+str(session_info.experiment_day)+'_'+probe+'_peak_waveforms.pkl')) and save_waveforms) \n",
    "                    and not overwrite and not surface_flag)):\n",
    "                        continue\n",
    "                device_timing_on_sync = npc_ephys.get_ephys_timing_on_sync(npc_lims.get_h5_sync_from_s3(session_info.id), \n",
    "                                                                            npc_lims.get_recording_dirs_experiment_path_from_s3(session_info.id),\n",
    "                                                                            only_devices_including='Probe'+probe)\n",
    "                \n",
    "                units=npc_ephys.make_units_table_from_spike_interface_ks25(session_info.id, device_timing_on_sync)\n",
    "                units=npc_ephys.add_global_unit_ids(units,session_info.id)\n",
    "\n",
    "                if surface_flag:\n",
    "\n",
    "                    units_surface=si_surface_channels.quality_metrics_df(probe=probe)\n",
    "                    units_surface=pd.concat([units_surface,si_surface_channels.template_metrics_df(probe=probe)],axis=1)\n",
    "                    units_surface['peak_channel']=npc_ephys.get_amplitudes_waveforms_channels_ks25(si_surface_channels, electrode_group_name=probe).peak_channels\n",
    "\n",
    "                    if units_surface['peak_channel'].max()<384:\n",
    "                        units_surface['peak_channel']+=384\n",
    "                    units_surface['electrode_group_name']='probe'+probe\n",
    "                    units_surface['cluster_id']=np.arange(0,len(units_surface))\n",
    "                    # add global unit IDs\n",
    "                    units_surface=npc_ephys.add_global_unit_ids(units_surface,surface_channel_session_id)\n",
    "                    #append to session units table\n",
    "                    units=pd.concat([units,units_surface],axis=0,ignore_index=True)\n",
    "\n",
    "\n",
    "                #add surface channel flag - or automatically deal with nans or no spikes\n",
    "                spike_utils.compute_metrics_for_alignment(trials, units, session_info, save_path)\n",
    "\n",
    "                #get waveforms for units\n",
    "                if save_waveforms:\n",
    "                    peak_waveforms={\n",
    "                        'unit_id':[],\n",
    "                        'session_id':[],\n",
    "                        'peak_channel':[],\n",
    "                        'probe':[],\n",
    "                        'waveform':[],\n",
    "                    }\n",
    "                    waveforms=si.get_nwb_units_device_property('waveform_mean','probe'+probe)\n",
    "                    if surface_flag:\n",
    "                        #get surface channel waveforms\n",
    "                        waveforms_surface=si_surface_channels.get_nwb_units_device_property('waveform_mean','probe'+probe)\n",
    "                        #append to session waveforms\n",
    "                        waveforms=np.concatenate([waveforms,waveforms_surface],axis=0)\n",
    "                            \n",
    "                    for uu,(_,unit) in enumerate(units.iterrows()):\n",
    "                        peak_waveforms['unit_id'].append(unit['unit_id'])\n",
    "                        peak_waveforms['session_id'].append(str(session_info.id))\n",
    "                        peak_waveforms['peak_channel'].append(unit['peak_channel'])\n",
    "                        peak_waveforms['probe'].append(probe)\n",
    "                        if unit['peak_channel']>383: \n",
    "                            #multi-channel waveform has 384, not 768 channels - subtract 384 from surface channels for correct index\n",
    "                            unit_peak_channel=unit['peak_channel']-384\n",
    "                        else:\n",
    "                            unit_peak_channel=unit['peak_channel']\n",
    "                        peak_waveforms['waveform'].append(waveforms[uu,:,unit_peak_channel])\n",
    "                    peak_waveforms=pd.DataFrame(peak_waveforms)\n",
    "                    peak_waveforms.to_pickle(os.path.join(save_path_wf,session_info.id+'_day_'+str(session_info.experiment_day)+'_'+probe+'_peak_waveforms.pkl'))\n",
    "\n",
    "                    if compute_waveform_PCs:\n",
    "                        pca_path=r\"\\\\allen\\programs\\mindscope\\workgroups\\dynamicrouting\\Ethan\\metrics for alignment\\waveform_pca_model.pkl\"\n",
    "                        pca_scaled_path=r\"\\\\allen\\programs\\mindscope\\workgroups\\dynamicrouting\\Ethan\\metrics for alignment\\waveform_pca_scaled_model.pkl\"\n",
    "\n",
    "                        waveforms_array=peak_waveforms['waveform'].values\n",
    "                        waveforms_array=np.vstack(waveforms_array)\n",
    "                        waveforms_array_scaled=waveforms_array/np.max(np.abs(waveforms_array),axis=1)[:,None]\n",
    "\n",
    "                        pca=pickle.load(open(pca_path,'rb'))\n",
    "                        pca_scaled=pickle.load(open(pca_scaled_path,'rb'))\n",
    "\n",
    "                        waveforms_transformed=pca.transform(waveforms_array)\n",
    "                        waveforms_scaled_transformed=pca_scaled.transform(waveforms_array_scaled)\n",
    "\n",
    "                        waveform_pcs={\n",
    "                            'unit_id':peak_waveforms['unit_id'].values,\n",
    "                            'session_id':peak_waveforms['session_id'].values,\n",
    "                            'peak_channel':peak_waveforms['peak_channel'].values,\n",
    "                        }\n",
    "\n",
    "                        #add PCs to dataframe\n",
    "                        for i in range(3):\n",
    "                            waveform_pcs['wf_PC'+str(i+1)]=waveforms_transformed[:,i]\n",
    "                        for i in range(3):\n",
    "                            waveform_pcs['wf_PC'+str(i+1)+'_scaled']=waveforms_scaled_transformed[:,i]\n",
    "\n",
    "                        waveform_pcs=pd.DataFrame(waveform_pcs)\n",
    "\n",
    "                        #hack to add probe as a columns\n",
    "                        probe_list=[]\n",
    "                        for xx in range(len(waveform_pcs)):\n",
    "                            probe_list.append('probe'+waveform_pcs['unit_id'].iloc[xx].split('_')[-1][0])\n",
    "                        waveform_pcs['probe']=probe_list\n",
    "\n",
    "                        #load metrics, merge, and re-save\n",
    "                        metrics=pd.read_csv(os.path.join(save_path,session_info.id+'_day_'+str(session_info.experiment_day)+'_probe'+probe+'_stim_modulation.csv'))\n",
    "                        metrics=metrics.merge(waveform_pcs,on=['unit_id','session_id','peak_channel','probe'])\n",
    "                        metrics.to_csv(os.path.join(save_path,session_info.id+'_day_'+str(session_info.experiment_day)+'_probe'+probe+'_stim_modulation.csv'),index=False)\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                except_dict[session_info.id+'_'+probe]=e\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        except_dict[session_info.id]=e\n",
    "\n",
    "#save except dict as pickle file\n",
    "with open(os.path.join(save_path,'except_dict.pkl'), 'wb') as f:\n",
    "    pickle.dump(except_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "except_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find expected vs. unexpected issues (doesn't account for multiple issues per session)\n",
    "known_issues_list={}\n",
    "for session in ephys_sessions: \n",
    "    if session.issues: \n",
    "        # print(session.id,session.issues)\n",
    "        known_issues_list[str(session.id)]=session.issues\n",
    "\n",
    "for session in except_dict.keys():\n",
    "    if session[:17] not in known_issues_list.keys():\n",
    "        print('unexpected issue:',session,except_dict[session])\n",
    "    else:\n",
    "        print('known issue:',session,known_issues_list[session[:17]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check which sessions have a metrics file, and which don't\n",
    "\n",
    "save_path=r\"\\\\allen\\programs\\mindscope\\workgroups\\dynamicrouting\\Ethan\\metrics for alignment\\stimulus responsiveness\"\n",
    "save_path_wf=r\"\\\\allen\\programs\\mindscope\\workgroups\\dynamicrouting\\Ethan\\metrics for alignment\\peak waveforms\"\n",
    "\n",
    "# ephys_sessions=tuple(s for s in npc_lims.get_session_info(is_ephys=True, is_uploaded=True,))\n",
    "metrics_file_list={\n",
    "    'session_id':[],\n",
    "    'day':[],\n",
    "    'probe':[],\n",
    "    'metrics_file':[],\n",
    "    'exists':[],\n",
    "}\n",
    "for session_info in ephys_sessions[:]:\n",
    "    try:\n",
    "        si = npc_ephys.SpikeInterfaceKS25Data(session_info.id)\n",
    "    \n",
    "        for probe in si.probes:\n",
    "            metrics_file_list['session_id'].append(session_info.id)\n",
    "            metrics_file_list['probe'].append(probe)\n",
    "            metrics_file_list['metrics_file'].append(session_info.id+'_day_'+str(session_info.experiment_day)+'_probe'+probe+'_stim_modulation.csv')\n",
    "            metrics_file_list['day'].append(session_info.experiment_day)\n",
    "            if os.path.exists(os.path.join(save_path,session_info.id+'_day_'+str(session_info.experiment_day)+'_probe'+probe+'_stim_modulation.csv')):\n",
    "                metrics_file_list['exists'].append(True)\n",
    "            else:\n",
    "                metrics_file_list['exists'].append(False)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "metrics_file_list=pd.DataFrame(metrics_file_list)\n",
    "\n",
    "metrics_file_list   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_file_list.to_csv(r'\\\\allen\\programs\\mindscope\\workgroups\\dynamicrouting\\Ethan\\metrics for alignment\\metrics_file_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_file_list.query('exists==False')[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load all metrics files and do PCA - no longer used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load waveforms, run PCA, then append to each metrics tables (PC1,2,3)\n",
    "\n",
    "waveform_path=r\"\\\\allen\\programs\\mindscope\\workgroups\\dynamicrouting\\Ethan\\metrics for alignment\\peak waveforms\"\n",
    "metrics_path=r\"\\\\allen\\programs\\mindscope\\workgroups\\dynamicrouting\\Ethan\\metrics for alignment\\stimulus responsiveness\"\n",
    "\n",
    "waveform_files=os.listdir(waveform_path)\n",
    "metrics_files=os.listdir(metrics_path)\n",
    "\n",
    "use_existing_pca_models=True\n",
    "pca_path=r\"\\\\allen\\programs\\mindscope\\workgroups\\dynamicrouting\\Ethan\\metrics for alignment\\waveform_pca_model.pkl\"\n",
    "pca_scaled_path=r\"\\\\allen\\programs\\mindscope\\workgroups\\dynamicrouting\\Ethan\\metrics for alignment\\waveform_pca_scaled_model.pkl\"\n",
    "\n",
    "#load waveforms and concat all \n",
    "for ww,wf_file in enumerate(waveform_files):\n",
    "    if 'peak_waveforms' in wf_file:\n",
    "        if ww==0:\n",
    "            waveforms=pd.read_pickle(os.path.join(waveform_path,wf_file))\n",
    "        else:\n",
    "            waveforms=pd.concat([waveforms,pd.read_pickle(os.path.join(waveform_path,wf_file))],axis=0)\n",
    "    \n",
    "#run PCA on waveforms\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "waveforms=waveforms.dropna()\n",
    "waveforms_array=waveforms['waveform'].values\n",
    "waveforms_array=np.vstack(waveforms_array)\n",
    "waveforms_array_scaled=waveforms_array/np.max(np.abs(waveforms_array),axis=1)[:,None]\n",
    "\n",
    "if use_existing_pca_models:\n",
    "    pca=pickle.load(open(pca_path,'rb'))\n",
    "    pca_scaled=pickle.load(open(pca_scaled_path,'rb'))\n",
    "else:\n",
    "    pca = PCA()\n",
    "    pca.fit(waveforms_array)\n",
    "    pca_scaled=PCA()\n",
    "    pca_scaled.fit(waveforms_array_scaled)\n",
    "\n",
    "waveforms_transformed=pca.transform(waveforms_array)\n",
    "waveforms_scaled_transformed=pca_scaled.transform(waveforms_array_scaled)\n",
    "\n",
    "waveform_pcs={\n",
    "    'unit_id':waveforms['unit_id'].values,\n",
    "    'session_id':waveforms['session_id'].values,\n",
    "    'peak_channel':waveforms['peak_channel'].values,\n",
    "}\n",
    "\n",
    "#add PCs to dataframe\n",
    "for i in range(3):\n",
    "    waveform_pcs['wf_PC'+str(i+1)]=waveforms_transformed[:,i]\n",
    "for i in range(3):\n",
    "    waveform_pcs['wf_PC'+str(i+1)+'_scaled']=waveforms_scaled_transformed[:,i]\n",
    "\n",
    "waveform_pcs=pd.DataFrame(waveform_pcs)\n",
    "\n",
    "#hack to add probe as a columns\n",
    "probe_list=[]\n",
    "for xx in range(len(waveform_pcs)):\n",
    "    probe_list.append('probe'+waveform_pcs['unit_id'].iloc[xx].split('_')[-1][0])\n",
    "waveform_pcs['probe']=probe_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save pca models\n",
    "# pca_path=r\"\\\\allen\\programs\\mindscope\\workgroups\\dynamicrouting\\Ethan\\metrics for alignment\\waveform_pca_model.pkl\"\n",
    "# pca_scaled_path=r\"\\\\allen\\programs\\mindscope\\workgroups\\dynamicrouting\\Ethan\\metrics for alignment\\waveform_pca_scaled_model.pkl\"\n",
    "\n",
    "# with open(pca_path, 'wb') as f:\n",
    "#     pickle.dump(pca, f)\n",
    "# with open(pca_scaled_path, 'wb') as f:\n",
    "#     pickle.dump(pca_scaled, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explained variance\n",
    "fig,ax=plt.subplots(1,1)\n",
    "ax.plot(pca.explained_variance_ratio_)\n",
    "ax.plot(pca_scaled.explained_variance_ratio_)\n",
    "ax.set_xlim([-1,10])\n",
    "ax.set_xlabel('PC')\n",
    "ax.set_ylabel('Explained variance')\n",
    "ax.set_title('Explained variance by PC')\n",
    "ax.legend(['raw waveforms','scaled waveforms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot top 3-5 PCs\n",
    "\n",
    "fig,ax=plt.subplots(1,1)\n",
    "\n",
    "# for i in range(10):\n",
    "#     ax.plot(waveforms_transformed[:,i],label=f'PC{i+1}')\n",
    "# for i in range(5):\n",
    "#     ax.plot(pca.components_[i,:],label=f'PC{i+1}',alpha=0.5)\n",
    "\n",
    "for i in range(5):\n",
    "    ax.plot(pca_scaled.components_[i,:],label=f'PC{i+1}',alpha=0.5)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('time (samples)')\n",
    "ax.set_ylabel('PC amplitude')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###append waveform PCs to metrics (waveform_pc1, etc)\n",
    "\n",
    "metrics_merge_path=r\"\\\\allen\\programs\\mindscope\\workgroups\\dynamicrouting\\Ethan\\metrics for alignment\\merged metrics\"\n",
    "metrics_path=r\"\\\\allen\\programs\\mindscope\\workgroups\\dynamicrouting\\Ethan\\metrics for alignment\\stimulus responsiveness\"\n",
    "metrics_merge_path=r\"\\\\allen\\programs\\mindscope\\workgroups\\dynamicrouting\\Ethan\\metrics for alignment\\merged metrics\"\n",
    "metrics_files=os.listdir(metrics_path)\n",
    "\n",
    "#loop through unique session ids in waveform_pcs df\n",
    "unique_sessions=waveform_pcs['session_id'].unique()\n",
    "\n",
    "for ss in unique_sessions[:]:\n",
    "    session_waveforms=waveform_pcs.query('session_id==@ss')\n",
    "\n",
    "    for probe in session_waveforms['probe'].unique():\n",
    "        #find matching file in metrics_path & load it\n",
    "        metrics_file=[mf for mf in metrics_files if (ss in mf)&(probe in mf)]\n",
    "        if len(metrics_file)>0:\n",
    "            metrics=pd.read_csv(os.path.join(metrics_path,metrics_file[0]))\n",
    "            #merge the session's metrics df with the waveform_pcs df on unit_id\n",
    "            if 'wf_PC1' in metrics.columns:\n",
    "                metrics.drop(columns=['wf_PC1','wf_PC2','wf_PC3'],inplace=True)\n",
    "            if 'wf_PC1_scaled' in metrics.columns:\n",
    "                metrics.drop(columns=['wf_PC1_scaled','wf_PC2_scaled','wf_PC3_scaled'],inplace=True)\n",
    "            metrics=metrics.merge(waveform_pcs,on=['unit_id','session_id','peak_channel','probe'])#,how='left')\n",
    "            # #save the merged df (in a new folder maybe?)\n",
    "            # metrics.to_csv(os.path.join(metrics_merge_path,metrics_file[0].replace('.csv','_merged.csv')),index=False)\n",
    "            metrics.to_csv(os.path.join(metrics_path,metrics_file[0]),index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try PCA on metrics themselves?\n",
    "\n",
    "# #load all metrics files, concatenate\n",
    "\n",
    "# metrics_files=os.listdir(metrics_path)\n",
    "# for mm,metrics_file in enumerate(metrics_files):\n",
    "#     if 'stim_modulation' in metrics_file:\n",
    "#         if mm==0:\n",
    "#             metrics=pd.read_csv(os.path.join(metrics_path,metrics_file))\n",
    "#         else:\n",
    "#             metrics=pd.concat([metrics,pd.read_csv(os.path.join(metrics_path,metrics_file))],axis=0)\n",
    "\n",
    "# metrics=metrics.dropna()\n",
    "\n",
    "#run PCA on metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "metrics_array=metrics.drop(columns=['unit_id','session_id','project','experiment_day','probe']).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(metrics_array)\n",
    "metrics_array=scaler.transform(metrics_array)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(metrics_array)\n",
    "\n",
    "metrics_transformed=pca.transform(metrics_array)\n",
    "\n",
    "#explained variance\n",
    "fig,ax=plt.subplots(1,1)\n",
    "ax.plot(pca.explained_variance_ratio_)\n",
    "ax.set_xlim([-1,10])\n",
    "ax.set_xlabel('PC')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,ax=plt.subplots(1,1)\n",
    "ax.plot(metrics['peak_channel'],metrics_transformed[:,2],'k.',alpha=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npc_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
