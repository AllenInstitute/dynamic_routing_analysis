{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode context from spikes or facemap\n",
    "\n",
    "1 - either use all annotated & uploaded ephys sessions as input or provide a list of session_ids\n",
    "\n",
    "2 - set a savepath and filename for the output - one .pkl file per session\n",
    "\n",
    "3 - set parameters - descriptions below\n",
    "\n",
    "4 - run decoding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append(r\"C:\\Users\\shailaja.akella\\Dropbox (Personal)\\DR\\dynamic_routing_analysis_ethan\\src\")\n",
    "\n",
    "import npc_lims\n",
    "from dynamic_routing_analysis import decoding_utils\n",
    "from npc_sessions import DynamicRoutingSession\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1A get all uploaded & annotated ephys sessions\n",
    "\n",
    "ephys_sessions = tuple(s for s in npc_lims.get_session_info(is_ephys=True, is_uploaded=True, \n",
    "                                                            is_annotated=True, project='DynamicRouting', issues = []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1B alternatively, provide a list of session ids:\n",
    "session_id_list=['712815_2024-05-22','708016_2024-05-01','664851_2023-11-14','702136_2024-03-05','686176_2023-12-05']\n",
    "session_list=[]\n",
    "for ss in session_id_list:\n",
    "    session_list.append(npc_lims.get_session_info(ss))\n",
    "ephys_sessions=tuple(session_list)\n",
    "ephys_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 set savepath and filename\n",
    "savepath=r\"\\\\allen\\programs\\mindscope\\workgroups\\templeton\\TTOC\\decoding results\\test\"\n",
    "filename='decoding_results_test'\n",
    "\n",
    "except_list={}\n",
    "\n",
    "#3 set parameters\n",
    "#linear shift decoding currently just takes the average firing rate over all bins defined here\n",
    "spikes_binsize=0.2 #bin size in seconds\n",
    "spikes_time_before=0.2 #time before the stimulus per trial\n",
    "spikes_time_after=0.01 #time after the stimulus per trial\n",
    "\n",
    "# #not used for linear shift decoding, were used in a previous iteration of decoding analysis\n",
    "# decoder_binsize=0.2\n",
    "# decoder_time_before=0.2\n",
    "# decoder_time_after=0.1\n",
    "\n",
    "\n",
    "params = {\n",
    "    'n_units': ['all'], #number of units to sample for each area (list)\n",
    "    'n_repeats': 25,  # number of times to repeat decoding with different randomly sampled units\n",
    "    'input_data_type': 'spikes',  # spikes or facemap or LP\n",
    "    'vid_angle_facemotion': 'face', # behavior, face, eye\n",
    "    'vid_angle_LP': 'behavior',\n",
    "    'central_section': '4_blocks_plus',\n",
    "    # for linear shift decoding, how many trials to use for the shift. '4_blocks_plus' is best\n",
    "    'exclude_cue_trials': False,  # option to totally exclude autorewarded trials\n",
    "    'n_unit_threshold': 20,  # minimum number of units to include an area in the analysis\n",
    "    'keep_n_SVDs': 500,  # number of SVD components to keep for facemap data\n",
    "    'LP_parts_to_keep': ['ear_base_l', 'eye_bottom_l', 'jaw', 'nose_tip', 'whisker_pad_l_side'],\n",
    "    'spikes_binsize': spikes_binsize,\n",
    "    'spikes_time_before': spikes_time_before,\n",
    "    'spikes_time_after': spikes_time_after,\n",
    "    # 'decoder_binsize':decoder_binsize,\n",
    "    # 'decoder_time_before':decoder_time_before,\n",
    "    # 'decoder_time_after':decoder_time_after,\n",
    "    'savepath': savepath,\n",
    "    'filename': filename,\n",
    "    'use_structure_probe': True,  # if True, appedn probe name to area name when multiple probes in the same area\n",
    "    'crossval': '5_fold',  # '5_fold' or 'blockwise' - blockwise untested with linear shift\n",
    "    'labels_as_index': True,  # convert labels (context names) to index [0,1]\n",
    "    'decoder_type': 'linearSVC',  # 'linearSVC' or 'LDA' or 'RandomForest' or 'LogisticRegression'\n",
    "    'only_use_all_units': False, #if True, do not run decoding with different areas, only with all areas -- for debugging\n",
    "}\n",
    "\n",
    "\n",
    "for ephys_session in ephys_sessions:\n",
    "    if os.path.exists(savepath + '/' + ephys_session.id[:17] + '_' + filename + '.pkl'): \n",
    "        print(ephys_session.id[:17] + ' completed, skipping...')    \n",
    "        continue\n",
    "    try:\n",
    "        session = DynamicRoutingSession(ephys_session.id)\n",
    "        print(session.id+' loaded')\n",
    "        if 'structure' in session.electrodes[:].columns:\n",
    "            session_info=ephys_session\n",
    "            session_id=str(session_info.id)\n",
    "            trials=pd.read_parquet(\n",
    "                npc_lims.get_cache_path('trials',session_id,'any')\n",
    "            )\n",
    "            units=pd.read_parquet(\n",
    "                npc_lims.get_cache_path('units',session_id,'any')\n",
    "            )\n",
    "            decoding_utils.decode_context_with_linear_shift(session=None,params=params,trials=trials,units=units,session_info=session_info)\n",
    "\n",
    "            #find path of decoder result\n",
    "            file_path= [os.path.join(savepath, ephys_session.id[:17] + '_' + filename + '.pkl')]\n",
    "\n",
    "            decoding_results=decoding_utils.concat_decoder_results(file_path,savepath=savepath,return_table=True,single_session=True)\n",
    "\n",
    "            #find n_units to loop through for next step\n",
    "            n_units=[]\n",
    "            for col in decoding_results.filter(like='true_accuracy_').columns.values:\n",
    "                if len(col.split('_'))==3:\n",
    "                    temp_n_units=col.split('_')[2]\n",
    "                    try:\n",
    "                        n_units.append(int(temp_n_units))\n",
    "                    except:\n",
    "                        n_units.append(temp_n_units)\n",
    "                else:\n",
    "                    n_units.append(None)\n",
    "\n",
    "            for nu in n_units:\n",
    "                decoding_utils.concat_trialwise_decoder_results(file_path,savepath=savepath,return_table=False,n_units=nu,single_session=True)\n",
    "\n",
    "        else:\n",
    "            print('no structure column found in electrodes table, moving to next recording')\n",
    "        session=[]\n",
    "    except Exception as e:\n",
    "        except_list[session.id]=repr(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savepath=r\"\\\\allen\\programs\\mindscope\\workgroups\\templeton\\TTOC\\decoding results\\test\"\n",
    "# filename='decoding_results_test'\n",
    "\n",
    "decoding_utils.concat_decoder_summary_tables(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "except_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
